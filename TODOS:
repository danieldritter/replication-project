10. Possibly rewrite reward function according to the code snippet linked in Philip's
email.

5. Write the while loop that interfaces with the engine,
    combines actor/critic loss, optimizes gradients etc. in 
    train a2c.py

7. compute #support, #X-support, #Eff-X-support
    #support = # of support orders given by a power in a game.
    #X-support = # of support orders given by a power in a game that helps FOREIGN powers
    #Eff-X-support = # of support orders given by a power in a game that helps FOREIGN powers CRITICALLY
    Calculating support:
        a. Look at orders

    Calculating Eff-X-support:
        a. Look at orders
        b. combine() function

8. Implement get_orders in AbstractActor
9. Put everything together in train.py

***************BLOCKED*****************
4. Write a loss function for the actor/encoder using 15-step return.
(blocked by Philip's response)
***************************************

***************DONE********************
1. Using the reward function, create labels for the value of each state (phase) in each game.
    a. Apply the reward function to all states in a game - DONE
    b. Run the discount function to get the values for each state (cumulative disc reward) - DONE
    c. Create new variable supply_centers_owners to hold the value at each state for each power. - DONE

2. Write the critic:
    a. Input: a state (e.g. one phase, 81 * 35 vector)
    b. Output: (7,) vector representing the value for each power.
        * mask caveat for games without 7 powers
    DONE

3. Pretrain the critic with each game; labels are the values computed in task 1.
    DONE

6. Create mask DONE
***************************************